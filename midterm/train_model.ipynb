{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c903e57bf0b9ccc5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/patrick/.cache/huggingface/datasets/json/default-c903e57bf0b9ccc5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d9a5d7b7984892b7d43d891a5c23bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e89093d8d436884cc68bc92ec8a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de62fa392fa424daa918fab25de1724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297aa2a88659489dbd7454048e214ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/patrick/.cache/huggingface/datasets/json/default-c903e57bf0b9ccc5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f602812e5f46bbbad5a5e3dd363d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files={'train': 'data/train.json', 'dev': 'data/dev.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['headline', 'label'],\n",
       "        num_rows: 74275\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['headline', 'label'],\n",
       "        num_rows: 15916\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2bc87210f644488cde24cc577d8eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/patrick/.cache/huggingface/datasets/json/default-c903e57bf0b9ccc5/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2eaf675232160678.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data['headline'], truncation=True)\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headline': \"'Bachelorette' Star Kaitlyn Bristowe Talks Cyberbullies And Death Threats On 'Men Tell All' Special\",\n",
       " 'label': 2,\n",
       " 'input_ids': [101,\n",
       "  112,\n",
       "  6143,\n",
       "  6347,\n",
       "  112,\n",
       "  2537,\n",
       "  13354,\n",
       "  8671,\n",
       "  1179,\n",
       "  139,\n",
       "  26691,\n",
       "  7921,\n",
       "  8768,\n",
       "  1116,\n",
       "  27688,\n",
       "  3169,\n",
       "  17719,\n",
       "  1905,\n",
       "  1262,\n",
       "  4735,\n",
       "  157,\n",
       "  8167,\n",
       "  13448,\n",
       "  1116,\n",
       "  1212,\n",
       "  112,\n",
       "  3401,\n",
       "  4630,\n",
       "  1398,\n",
       "  112,\n",
       "  3139,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.distributed' has no attribute 'is_initialized'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn [51], line 3\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n",
      "\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m<string>:91\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, data_seed, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, gradient_checkpointing, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters)\u001b[0m\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/training_args.py:865\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    857\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n",
      "\u001b[1;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`--adafactor` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--optim adafactor` instead\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[1;32m    859\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n",
      "\u001b[1;32m    860\u001b[0m     )\n",
      "\u001b[1;32m    861\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim \u001b[39m=\u001b[39m OptimizerNames\u001b[39m.\u001b[39mADAFACTOR\n",
      "\u001b[1;32m    863\u001b[0m \u001b[39mif\u001b[39;00m (\n",
      "\u001b[1;32m    864\u001b[0m     is_torch_available()\n",
      "\u001b[0;32m--> 865\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m    866\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mGPU_NUM_DEVICES\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron)\n",
      "\u001b[1;32m    867\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n",
      "\u001b[1;32m    868\u001b[0m ):\n",
      "\u001b[1;32m    869\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    871\u001b[0m     )\n",
      "\u001b[1;32m    873\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf32 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/utils/import_utils.py:781\u001b[0m, in \u001b[0;36mtorch_required.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    778\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n",
      "\u001b[1;32m    779\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_available():\n",
      "\u001b[0;32m--> 781\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m    782\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m    783\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMethod `\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` requires PyTorch.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/training_args.py:1099\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1093\u001b[0m \u001b[39m@property\u001b[39m\n",
      "\u001b[1;32m   1094\u001b[0m \u001b[39m@torch_required\u001b[39m\n",
      "\u001b[1;32m   1095\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdevice\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtorch.device\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;32m   1096\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   1097\u001b[0m \u001b[39m    The device used by this process.\u001b[39;00m\n",
      "\u001b[1;32m   1098\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1099\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/utils/generic.py:48\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n",
      "\u001b[1;32m     46\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m---> 48\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n",
      "\u001b[1;32m     49\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n",
      "\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/utils/import_utils.py:781\u001b[0m, in \u001b[0;36mtorch_required.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    778\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n",
      "\u001b[1;32m    779\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_available():\n",
      "\u001b[0;32m--> 781\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m    782\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m    783\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMethod `\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` requires PyTorch.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/training_args.py:1024\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1020\u001b[0m \u001b[39m@cached_property\u001b[39m\n",
      "\u001b[1;32m   1021\u001b[0m \u001b[39m@torch_required\u001b[39m\n",
      "\u001b[1;32m   1022\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_setup_devices\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtorch.device\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;32m   1023\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mPyTorch: setting up devices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m-> 1024\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mdistributed\u001b[39m.\u001b[39;49mis_initialized() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_rank \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "\u001b[1;32m   1025\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n",
      "\u001b[1;32m   1026\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorch.distributed process group is initialized, but local_rank == -1. \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   1027\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIn order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   1028\u001b[0m         )\n",
      "\u001b[1;32m   1029\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_cuda:\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.distributed' has no attribute 'is_initialized'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn [59], line 5\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n",
      "\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n",
      "\u001b[1;32m      4\u001b[0m     model,\n",
      "\u001b[0;32m----> 5\u001b[0m     \u001b[43mtraining_args\u001b[49m,\n",
      "\u001b[1;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m      7\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m      8\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n",
      "\u001b[1;32m      9\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n",
      "\u001b[1;32m     10\u001b[0m )\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_args' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
